{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ITMAL Exercise\n",
    "\n",
    "REVISIONS||\n",
    "---------||\n",
    "2018-1219| CEF, initial.                  \n",
    "2018-0206| CEF, updated and spell checked. \n",
    "2018-0208| CEF, minor text update.\n",
    "2018-0305| CEF, updated with SHN comments.\n",
    "2019-0902| CEF, updated for ITMAL v2.\n",
    "2019-0904| CEF, updated and added conclusion Q.\n",
    "\n",
    "## Implementing a dummy classifier with fit-predict interface\n",
    "\n",
    "We begin with the MNIST data-set and will reuse the data loader from Scikit-learn. Next we create a dummy classifier, and compare the results of the SGD and dummy classifiers using the MNIST data...\n",
    "\n",
    "#### Qb  Load and display the MNIST data\n",
    "\n",
    "There is a `sklearn.datasets.fetch_openml` dataloader interface in Scikit-learn. You can load MNIST data like \n",
    "\n",
    "```python\n",
    "from sklearn.datasets import fetch_openml\n",
    "# Load data from https://www.openml.org/d/554\n",
    "X, y = fetch_openml('mnist_784',??) # needs to return X, y, replace '??' with suitable parameters! \n",
    "# Convert at scale (not always needed)\n",
    "#X = X / 255.\n",
    "```\n",
    "\n",
    "but you need to set parameters like `return_X_y` and `cache` if the default values are not suitable! \n",
    "\n",
    "Check out the documentation for the `fetch_openml` MNIST loader, try it out by loading a (X,y) MNIST data set, and plot a single digit via the `MNIST_PlotDigit` function here (input data is a 28x28 NMIST subimage)\n",
    "\n",
    "```python\n",
    "%matplotlib inline\n",
    "def MNIST_PlotDigit(data):\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "    image = data.reshape(28, 28)\n",
    "    plt.imshow(image, cmap = matplotlib.cm.binary, interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "```\n",
    "\n",
    "Finally, put the MNIST loader into a single function called `MNIST_GetDataSet()` so you can resuse it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "# Load data from https://www.openml.org/d/554\n",
    "X, y = fetch_openml('mnist_784',return_X_y=True)\n",
    "# Convert at scale (not always needed)\n",
    "X = X / 255.\n",
    "\n",
    "def MNIST_PlotDigit(data):\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "    image = data.reshape(28, 28)\n",
    "    plt.imshow(image, cmap = matplotlib.cm.binary, interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "MNIST_PlotDigit(X[0])    \n",
    "    \n",
    "def MNIST_GetDataSet(X, y):\n",
    "    X, y = fetch_openml('mnist_784',return_X_y=True)\n",
    "    X = X / 255.\n",
    "    \n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qb  Add a Stochastic Gradient Decent [SGD] Classifier\n",
    "\n",
    "Create a train-test data-set for MNIST and then add the `SGDClassifier` as done in [HOLM], p82.\n",
    "\n",
    "Split your data and run the fit-predict for the classifier using the MNIST data.(We will be looking at cross-validation instead of the simple fit-predict in a later exercise.)\n",
    "\n",
    "Notice that you have to reshape the MNIST X-data to be able to use the classifier. It may be a 3D array, consisting of 70000 (28 x 28) images, or just a 2D array consisting of 70000 elements of size 784.\n",
    "\n",
    "A simple `reshape()` could fix this on-the-fly:\n",
    "```python\n",
    "X, y = MNIST_GetDataSet()\n",
    "\n",
    "print(f\"X.shape={X.shape}\") # print X.shape= (70000, 28, 28)\n",
    "if X.ndim==3:\n",
    "    print(\"reshaping X..\")\n",
    "    assert y.ndim==1\n",
    "    X = X.reshape((X.shape[0],X.shape[1]*X.shape[2]))\n",
    "assert X.ndim==2\n",
    "print(f\"X.shape={X.shape}\") # X.shape= (70000, 784)\n",
    "```\n",
    "\n",
    "Remember to use the category-5 y inputs\n",
    "\n",
    "```python\n",
    "y_train_5 = (y_train == '5')    \n",
    "y_test_5  = (y_test == '5')\n",
    "```\n",
    "instead of the `y`'s you are getting out of the dataloader...\n",
    "\n",
    "Test your model on using the test data, and try to plot numbers that have been categorized correctly. Then also find and plots some misclassified numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.9765\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "#Creating the training and test datasets.\n",
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\n",
    "\n",
    "#Shuffle dataset\n",
    "shuffle_index = np.random.permutation(60000)\n",
    "X_train, y_train = X_train[shuffle_index], y_train[shuffle_index]\n",
    "\n",
    "#Creating the binary classifier for 5!\n",
    "y_train_5 = (y_train == '5')    \n",
    "y_test_5  = (y_test == '5')\n",
    "\n",
    "#Fit dataset\n",
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "sgd_clf.fit(X_train, y_train_5)\n",
    "y_pred = sgd_clf.predict(X_test)\n",
    "\n",
    "print('Accuracy score:', accuracy_score(y_test_5, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for X_test[15]:  [ True]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Predict and print a 5\n",
    "pred5 = sgd_clf.predict([X_test[15]])\n",
    "print('Prediction for X_test[15]: ',pred5)\n",
    "MNIST_PlotDigit(X_test[15])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for X_test[15]:  [False]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Predict and print a non 5\n",
    "pred9 = sgd_clf.predict([X_test[16]])\n",
    "print('Prediction for X_test[15]: ',pred9) \n",
    "MNIST_PlotDigit(X_test[16])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qc Implement a dummy binary classifier\n",
    "\n",
    "Now we will try to create a Scikit-learn compatible estimator implemented via a python class. Follow the code found in [HOML], p84, but name you estimator `DummyClassifier` instead of `Never5Classifyer`.\n",
    "\n",
    "Here our Python class knowledge comes into play. The estimator class hierarchy looks like\n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/E19_itmal/L02/Figs/class_base_estimator.png\" style=\"width:500px\">\n",
    "\n",
    "All Scikit-learn classifiers inherit from `BaseEstimator` (and possibly also `ClassifierMixin`), and they must have a `fit-predict` function pair (strangely not in the base class!) and you can actually find the `sklearn.base.BaseEstimator` and `sklearn.base.ClassifierMixin` python source code somewhere in you anaconda install dir, if you should have the nerves to go to such interesting details.\n",
    "\n",
    "But surprisingly you may just want to implement a class that contains the `fit-predict` functions, ___without inheriting___ from the `BaseEstimator`, things still work due to the pythonic 'duck-typing': you just need to have the class implement the needed interfaces, obviously `fit()` and `predict()` but also the more obscure `get_params()` etc....then the class 'looks like' a `BaseEstimator`...and if it looks like an estimator, it _is_ an estimator (aka. duck typing).\n",
    "\n",
    "Templates in C++ also allow the language to use compile-time duck typing!\n",
    "\n",
    "> https://en.wikipedia.org/wiki/Duck_typing\n",
    "\n",
    "Call the fit-predict on a newly instantiated `DummyClassifier` object, and find a way to extract the accuracy `score` from the test data. You may implement an accuracy function yourself or just use the `sklearn.metrics.accuracy_score` function. \n",
    "\n",
    "Finally, compare the accuracy score from your `DummyClassifier` with the scores found in [HOML] \"Measuring Accuracy Using Cross-Validation\", p.83. Are they comparable? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for X_test[15]:  [False]\n",
      "Accuracy score: 0.9108\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class DummyClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "    def predict(self, X):\n",
    "        return np.zeros((len(X), 1), dtype=bool)\n",
    "\n",
    "#Fit dataset\n",
    "dum_clf = DummyClassifier()\n",
    "dum_clf.fit(X_train, y_train_5)\n",
    "\n",
    "#Predict and print a 5\n",
    "dum_pred5 = dum_clf.predict(X_test)\n",
    "print('Prediction for X_test[15]: ',dum_pred5[15])\n",
    "MNIST_PlotDigit(X_test[15])    \n",
    "\n",
    "print('Accuracy score:', accuracy_score(y_test_5, dum_pred5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy scoren ligger lavere end dem på s. 83 i HOML. Men de er dog stadig sammenlignelige. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qd Conclusion\n",
    "\n",
    "Now, conclude on all the exercise above. \n",
    "\n",
    "Write a short textual conclusion (max. 10- to 20-lines) that extract the _essence_ of the exercises: why did you think it was important to look at these particular ML concepts, and what was our overall learning outcome of the exercises (in broad terms)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvis man kigger på nøjagtigheden for de to classifiers, så er det nemt at se, at SGDClassifier har en bedre nøjagtighed end den selvimplementerede DummyClassifier. SGDClassifier har en nøjagtiged på 0.978, mens DummyClassifier kun har en nøjagtighed på 0.9108. Man skal dog her være opmærksom på, at denne nøjagtighed kan variere fra gang til gang, når man fitter sit datasæt. \n",
    "Man kan også se på de tal, som er blevet udvalgt, at SGDClassifier har gættet rigtig på X_test[15] (og korrekt klassificeret dette som et 5 tal), men at DummyClassifier ikke klassificeret dette korrekt. (Hvilket også giver god mening, da den altid giver negativ)\n",
    "\n",
    "Læringen fra denne øvelse er, at nogen klassifier er bedre end andre til forskellige ting. Det kan derfor være en god idé at tjekke dem op imod hinanden, og finde den bedste på SIT datasæt. Dette kan dog godt tage længere tid, men som sæt af DummyClassifier og SGDClassifier, så er der forskel på nøjagtigheden, hvor man her vil vælge SGDClassifier over DummyClassifier. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
